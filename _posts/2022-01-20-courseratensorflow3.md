---
layout: single #postë„ ê°€ëŠ¥
title:  "Deeplearning.Ai TensorFlow Developer (Course 3)"
---

** [ì•Œë¦¼] **  <br>
ğŸ’â€â™€ï¸ í…ì„œí”Œë¡œìš° ìê²©ì¦ ì·¨ë“ì— ë„ì›€ì´ ë˜ëŠ” **ì½”ì„¸ë¼** ê°•ì˜ <br>
ğŸ’» ["Deeplearning.Ai TensorFlow Developer"](https://www.coursera.org/professional-certificates/tensorflow-in-practice?trk_ref=globalnav) - Course 3 : Natural Language Processingì„ ë“£ê³  ê°•ì˜ ë‚´ìš©ì„ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.<br>
ğŸ§  ìˆ˜ì—…ì„ ë“¤ìœ¼ë©° ë™ì‹œì— ì •ë¦¬í•œ ë‚´ìš©ì´ì–´ì„œ(í•„ê¸°ë…¸íŠ¸ ëŒ€ìš©), ì˜ì‹ì˜ íë¦„ì´ ê°•í•˜ê²Œ ê°œì…ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
ğŸ˜š ì €ë§Œì˜ ì´í•´ ë°©ë²•ì„ í’€ì–´ ë†“ì•„, ê°•ì˜ì™€ í•¨ê»˜ ë³´ì‹œëŠ” ë¶„ê»˜ëŠ” ì‘ì€ ë„ì›€ì´ ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.<br>

# Week 1. Sentiment in Text
## Lectures
### Using APIs for NLP

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)

#ê²°ê³¼: {'i':3, ....., 'dog':4}
```

### Lab 1. tokenize_basic
>  how to tokenize the words and sentences, building up a dictionary of all the words to make a corpus(ë§ë­‰ì¹˜)
* link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W1/ungraded_labs/C3_W1_Lab_1_tokenize_basic.ipynb

* `sequences=tokenizer.texts_to_sequences(sentences)` í•˜ë©´, ë¬¸ì¥ì´ encodingëœ ëª¨ì–‘ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤. [1,2,4,5] ì´ë ‡ê²Œ.
ê·¼ë° ì´ì œ fit_on_texts()ì—ì„œ indexingí•˜ì§€ ì•Šì€ ë‹¨ì–´ëŠ” ì•ˆë‚˜ì˜¤ê²Œ ëœë‹¤.
* ì´ë¡œì¨ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒ.
1. ë§ì€ ë‹¨ì–´ëŸ‰ì„ í™•ë³´í•˜ê¸° ìœ„í•´ì„œ ì•„ì£¼ í° training data ê°€ í•„ìš”í•˜ë‹¤.
2. ë³´ì§€ ëª»í•œ ë‹¨ì–´ê°€ ë‚˜ì˜¨ë‹¤ë©´ ê·¸ëƒ¥ ë¬´ì‹œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì–´ë–¤ íŠ¹ì • ê°’ì„ ë§¤ê²¨ì£¼ëŠ” ê²ƒì´ ì¢‹ì•„ë³´ì¸ë‹¤.

### Lab 2. sequences_basic
* Link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W1/ungraded_labs/C3_W1_Lab_2_sequences_basic.ipynb

```python
import tensorflow as tf
from tensorflow import keras


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences #ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ë“¤ì„ ì´ìš©í•  ë•Œ, 
# ë‹¤ ê°™ì€ ê¸¸ì´ì˜ ë¬¸ì¥ì´ ë˜ë„ë¡ ì§§ì€ ê²ƒì€ paddingí•˜ê±°ë‚˜ ê¸´ ê²ƒì€ truncate í•´ì£¼ëŠ” ê¸°ëŠ¥.


sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>") # oov_token: word indexì—ì„œ ì¸ì‹ ì•ˆë˜ëŠ” ë‹¨ì–´ë¥¼ ìœ„í•´ special tokenë„ ë§Œë“¤ê² ë‹¤
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

# ê·¸ë¦¬ê³  ì´ì œ ë¬¸ì¥ ë‚´ì˜ ë‹¨ì–´ë“¤ì„ sequences of tokensìœ¼ë¡œ ì „í™˜í•˜ê¸° by calling 'texts_to_sequences()' method
sequences = tokenizer.texts_to_sequences(sentences)

padded = pad_sequences(sequences, maxlen=5) # max lengthë¥¼ 5 wordsë¡œ, ê·¸ëŸ¬ë©´ 4 words ë¬¸ì¥ì€ pre-paddedë˜ì–´ 0 * * * * ë¨.
# 6 words ë¬¸ì¥ì€ ì•ì— í•œ ë‹¨ì–´ê°€ erase ëœë‹¤.
print("\nWord Index = " , word_index)
print("\nSequences = " , sequences)
print("\nPadded Sequences:")
print(padded)


# Try with words that the tokenizer wasn't fit to
test_data = [
    'i really love my dog',
    'my dog loves my manatee'
]

test_seq = tokenizer.texts_to_sequences(test_data)
print("\nTest Sequence = ", test_seq)

padded = pad_sequences(test_seq, maxlen=10)
print("\nPadded Test Sequence: ")
print(padded)

```

### Sarcasm, really?

```python
#json íŒŒì¼ì„ íŒŒì´ì¬ êµ¬ì¡°ë¡œ ë§Œë“œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
import json # allows you to load data in JSON format and automatically create a Python data structure from it.

with open("/sarcasm.json", 'r') as f:
    datastore = json.load(f)

sentences = [] 
labels = []
urls = []
for item in datastore:
    sentences.append(item['headline'])
    labels.append(item['is_sarcastic'])
    urls.append(item['article_link'])
```
### lab 3. sarcasm
* link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W1/ungraded_labs/C3_W1_Lab_3_sarcasm.ipynb

```python
word_index = tokenizer.word_index
print(len(word_index)) #ì¸ë±ìŠ¤ê°€ ëª‡ ê°œë‚˜ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€
print(word_index) # order of commonalityë¡œ ìˆ«ìê°€ ë§¤ê²¨ì§„ë‹¤.
sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences, padding='post')
print(padded[0])
print(padded.shape) #26709 sentences in the dataset and they are 40 characters long
```
## Quiz

1. What is the name of the object used to tokenize sentences?
-> Tokenizer

1. What is the name of the method used to tokenize a list of sentences?
-> fit_on_texts(sentences)

1. Once you have the corpus tokenized, whatâ€™s the method used to encode a list of sentences to use those tokens?
-> texts_to_sequences(sentences)

1. When initializing the tokenizer, how to you specify a token to use for unknown words?
-> oov_token=<Token>

1. If you donâ€™t use a token for out of vocabulary words, what happens at encoding?
-> The word isnâ€™t encoded, and is skipped in the sequence

1. If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?
-> Make sure that they are all the same length using the pad_sequences method of the tokenizer

1. If you have a number of sequences of different length, and call pad_sequences on them, whatâ€™s the default result?
-> Theyâ€™ll get padded to the length of the longest sequence by adding zeros to the beginning of shorter ones

1. When padding sequences, if you want the padding to be at the end of the sequence, how do you do it?
-> Pass padding=â€™postâ€™ to pad_sequences when initializing it

## Optional Assignment- Explore the BBC news archive (í•´ë³´ê¸°)
> For this exercise youâ€™ll get the BBC text archive. Your job will be to tokenize the dataset, removing common stopwords. A great source of these stop words can be found here.
* [BBC text archive](http://mlg.ucd.ie/datasets/bbc.html)
* [A great source of these stop words](https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js)
* [Solution](https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W1/assignment/C3_W1_Assignment_Solution.ipynb)

# Week 2. Word Embedding
> you'll take that to the next step using something called **Embeddings**, that takes these numbers and starts to establish sentiment(ì •ì„œ, ê°ì •) from them, so that you can begin to classify and then later predict texts.
## Lectures
### IMBD review dataset
> You will find here 50,000 movie reviews which are classified as positive of negative. 
* http://ai.stanford.edu/~amaas/data/sentiment/

### Lab1. 
* link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W2/ungraded_labs/C3_W2_Lab_1_imdb.ipynb

```python
!pip install tensorflow==2.5.0

import tensorflow as tf
print(tf.__version__)

#!pip install -q tensorflow-datasets

import tensorflow_datasets as tfds
imdb, info = tfds.load("imdb_reviews", with_info=True, as_supervised=True) #imdb_reviewsê°€ì ¸ì˜¤ê¸°

import numpy as np

train_data, test_data = imdb['train'], imdb['test'] # ê°ê° ë‚˜ëˆ ì¤¬ìŒ

training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = []

# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()
for s,l in train_data:
  training_sentences.append(s.numpy().decode('utf8'))
  training_labels.append(l.numpy())
  
for s,l in test_data:
  testing_sentences.append(s.numpy().decode('utf8'))
  testing_labels.append(l.numpy())
  
training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)

vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type='post'
oov_tok = "<OOV>"


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

print(decode_review(padded[3]))
print(training_sentences[3])

### How can we use vectors?
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Flatten(),
    #tf.keras.layers.GlobalAveragePooling1D(), #ë„ flatten ëŒ€ì‹  ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤. output shape = 16 (simpleí•˜ê³  ì¡°ê¸ˆ ë” ë¹ ë¦„)
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


### More into the details
num_epochs = 10
model.fit(padded,
    training_labels_final, 
    epochs=num_epochs,
    validation_data=(testing_padded, testing_labels_final))

e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)

#write the vector and metadata auto files.
import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()

try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')

# and go to the tensorflow rending projector on projector.tensorflow.org

out_m.close()

```
### Lab2. sarcasm_classifier

* Link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/20_sep_2021_fixes/C3/W2/ungraded_labs/C3_W2_Lab_2_sarcasm_classifier.ipynb#scrollTo=2HYfBKXjkmU8
<br> <br>  
* TensorFlow Dataset Catalog : https://www.tensorflow.org/datasets/catalog/overview
* TensorFlow Dataset : https://github.com/tensorflow/datasets/tree/master/docs/catalog
* IMDB review Dataset : https://github.com/tensorflow/datasets/blob/master/docs/catalog/imdb_reviews.md
* Subword text Encoder : https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder


### Diving into the code (part 2)
subwords ë“¤ì€ í•œë° ë¬¶ì–´ì„œ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´ë˜ì–´ì•¼ ì˜ë¯¸ìˆì–´ì§€ë‹ˆê¹Œ (semantic), ì´ì œê¹Œì§€ëŠ” ê·¸ ìœ„ì¹˜(ordering)ëŠ” ì¤‘ìš”í•˜ê²Œ ì—¬ê¸°ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ê±°ê¸°ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì œ ë•Œë¬¸ì— loss ê°€ ëŠ˜ì–´ë‚œë‹¤...
ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì£¼ 3ì°¨ì— recurrent Neural Network ë°°ìš¸ ê²ƒ

### lab3. imdb_subwords
* link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W2/ungraded_labs/C3_W3_Lab_3_imdb_subwords.ipynb

## Optional Assignment- BBC news archive (í•´ë³´ê¸°2)
* link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W2/assignment/C3_W2_Assignment.ipynb
* Solution link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W2/assignment/C3_W2_Assignment_Solution.ipynb

# Week 3. Sequence model
## Lectures
### RNN - Introduction
### RNN ê°œë… í•™ìŠµ ì˜ìƒ
* ì•¤ë“œë¥˜ì˜ Sequence modeling : https://www.coursera.org/lecture/nlp-sequence-models/deep-rnns-ehs0S

### LSTM(Long Short Term Memory)
Today has a beautiful <>
<> might be sky
> In addition to the context being PaaSed as it is in RNNs, LSTMs have an additional pipeline of contexts called cell state. This can pass through the network to impact it. This helps keep context from earlier tokens relevance in later ones so issues like the one that we just discussed can be avoided

> Cell states can also be bidirectional. So later contexts can impact earlier ones as we'll see when we look at the code. 

* LSTM ê°•ì˜: https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay

### Implementing LSTMs in code (lab1, lab2)
* lab1_single_layer: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_1_single_layer_LSTM.ipynb
* lab2_multiple_layer: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb

```python
# ONE LAYER SLTM
dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']
tokenizer = info.features['text'].encoder
BUFFER_SIZE = 10000
BATCH_SIZE = 64

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))
test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), # it will make my cell state go in both directions
# í˜¹ì‹œ LSTMì„ ë‘ ê²¹ ì´ìƒìœ¼ë¡œ ìŒ“ê²Œ ë˜ë©´ ì²«ë²ˆì§¸ LSTM ë¼ì¸ì— "return_sequences = True" í•´ì¤˜ì•¼í•œë‹¤. This ensures that the outputs of the LSTM match the desired inputs of the next one.(LSTMì˜ ì¶œë ¥ê°’ ëª¨ì–‘ì´ ë‹¤ìŒ LSTMì˜ ì…ë ¥ëª¨ì–‘ì´ë‘ ì„œë¡œ ë§ê²Œ í•´ì¤Œ)
 
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.summary()
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
NUM_EPOCHS = 10
history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)
import matplotlib.pyplot as plt

# TWO LAYERS SLTM
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences = True))
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

### Accuracy and loss
> Let's look at the impact of using an LSTM on the model that we looked at in the last module, where we had subword tokens.

>  the comparison of accuracies between the one layer LSTM and the two layer one over 10 epochs. There's not much of a difference except **the nosedive(ê¸‰ê°•í•˜) of the validation accuracy.**

>  I found from training networks **that jaggedness(ë“¤ì­‰ë‚ ì­‰í•¨) can be an indication that your model needs improvement**, and the single LSTM that you can see here is not the smoothest.

### A word from Laurence
> In this video, you'll see some other options of RNN including convolutions, Gated Recurrent Units also called GRUs, and more on how you can write the code for them. You'll investigate the impact that they have on training

* ì‹¤í—˜
> But we can experiment with the layers that bridge the embedding and the dense by removing the flatten and puling from here, and replacing them with an LSTM like this.
* ê²°ê³¼
> Again, this shows some over fitting in the LSTM network. While the accuracy of the prediction increased, the confidence in it decreased. So you should be careful to adjust your training parameters when you use different network types, it's not just a straight drop-in like I did here

### embedding í›„ì— Conv network ë„£ê¸°

> ???? :  If we go back to the model and explore the parameters, we'll see that we have 128 filters each for 5 words. And an exploration of the model will show these dimensions. As the size of the input was 120 words, and a filter that is 5 words long will shave off 2 words from the front and back, leaving us with 116. The 128 filters that we specified will show up here as part of the convolutional layer.

### lab 3. multiple_layer_GRU
* link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_3_multiple_layer_GRU.ipynb

ì¼ë°˜ ë°©ë²•, RNN- LSTM ë°©ë²•, RNN-GRU ë°©ë²•, Conv(+pooling1d) ë°©ë²• ì¨ë´„
ëŒ€ë¶€ë¶„ overfitting ë¬¸ì œê°€ ë‚˜íƒ€ë‚¨


### lab 4. imdb_reviews_with_GRU_LSTM_Conv1D.
*link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_4_imdb_reviews_with_GRU_LSTM_Conv1D.ipynb

### ì˜¤ë²„í”¼íŒ…ì„ í”¼í•˜ê¸° ìœ„í•œ tips from Laurance
> Try them out for yourself, check on the time, check on the results, and see **what techniques you can figure out to avoid some of the overfitting.** 

> Remember that with text, you'll probably get a bit more overfitting than you would have done with images. Not least because you'll almost always have out of vocabulary words in the validation data set. That is words in the validation dataset that weren't present in the training, naturally leading to overfitting. These words can't be classified and, of course, you're going to have these overfitting issues, but see what you can do to avoid them.

### lab 5 - lab 6
> We've created a number of notebooks for you to explore the different types of sequence model.   Spend some time going through these to see how they work, and what the impact of different layer types have on training for classification.

* Sarcasm with Bidirectional LSTM (lab 5) : https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_5_sarcasm_with_bi_LSTM.ipynb

* Sarcasm with 1D Convolutional Layer (lab 6) :  https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_6_sarcasm_with_1D_convolutional.ipynb

## Quiz

1. Why does sequence make a large difference when determining semantics of language?
-> Because the order in which words appear dictate their meaning

1. How do Recurrent Neural Networks help you understand the impact of sequence on meaning?
-> They carry meaning from one cell to the next

1. How does an LSTM help understand meaning when words that qualify each other arenâ€™t necessarily beside each other in a sentence?
-> Values from earlier words can be carried to later ones via a cell state

1. What keras layer type allows LSTMs to look forward and backward in a sentence?
-> Bidirectional

1. Whatâ€™s the output shape of a bidirectional LSTM layer with 64 units?
-> (None, 128)

1. When stacking LSTMs, how do you instruct an LSTM to feed the next one in the sequence?
-> Ensure that return_sequences is set to True only on units that feed to another LSTM

1. If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, whatâ€™s the output shape?   
**-> (None, 116, 128)***

1. Whatâ€™s the best way to avoid overfitting in NLP datasets?
-> None of the above (Use LSTMs, Use GRUs, Use Conv1D)
## Wrap up - week 3.
> Youâ€™ve been experimenting with NLP for text classification over the last few weeks. Next week youâ€™ll switch gears -- and take a look at using the tools that youâ€™ve learned to predict text, which ultimately means you can create text. By learning sequences of words you can predict the most common word that comes next in the sequence, and thus, when starting from a new sequence of words you can create a model that builds on them. Youâ€™ll take different training sets -- like traditional Irish songs, or Shakespeare poetry, and learn how to create new sets of words using their embeddings!

(í•´ì„: ì§€ë‚œ ëª‡ ì£¼ ë™ì•ˆ í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•´ NLPë¥¼ ì‹¤í—˜í•´ ì™”ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì£¼ì—ëŠ” ê¸°ì–´ë¥¼ ë°”ê¿”ì„œ í•™ìŠµí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì´ëŠ” ê¶ê·¹ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ í•™ìŠµí•˜ë©´ ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒì— ì˜¤ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒˆë¡œìš´ ë‹¨ì–´ ì‹œí€€ìŠ¤ì—ì„œ ì‹œì‘í•  ë•Œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „í†µì ì¸ ì•„ì¼ëœë“œ ë…¸ë˜ë‚˜ ì…°ìµìŠ¤í”¼ì–´ ì‹œì™€ ê°™ì€ ë‹¤ì–‘í•œ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìˆ˜ê°•í•˜ê³  ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë‹¨ì–´ ì„¸íŠ¸ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤!)


## Optional Assignment: Exploring overfitting in NLP (í•´ë³´ê¸°)

* When looking at a number of different types of layer for text classification this week you saw many examples of overfitting -- with one of the major reasons for the overfitting being that your training dataset was quite small, and with a small number of words. Embeddings derived from this may be over generalized also. So for this weekâ€™s exercise youâ€™re going to train on a large dataset, as well as using transfer learning of an existing set of embeddings.

* The dataset is from:  https://www.kaggle.com/kazanova/sentiment140. Iâ€™ve cleaned it up a little, in particular to make the file encoding work with Python CSV reader. 

* The embeddings that you will transfer learn from are called the GloVe, also known as Global Vectors for Word Representation, available at: https://nlp.stanford.edu/projects/glove/

* Link assignment: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W3/assignment/C3_W3_Assignment.ipynb
* Link Solution : https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W3/assignment/C3_W3_Assignment_Solution.ipynb

# Week 4. Sequence Model and Literature
## Lectures
### Lab 1. Intro for 'Create Texts exercise'
* lab.1 link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W4/ungraded_labs/C3_W4_Lab_1.ipynb

lil.ai enter
### ë¡œë ŒìŠ¤ ì‹œ ë…¸íŠ¸ë¶
* ë§í¬: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W4/misc/Laurences_generated_poetry.txt

### ì•„ì´ë¦¬ì‹œ ê°€ì‚¬ ë…¸íŠ¸ë¶
* ë§í¬: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W4/ungraded_labs/C3_W4_Lab_2_irish_lyrics.ipynb

### ë¬¸ì(character) ê¸°ë°˜í•œ RNN (ë¹„êµ: ë‹¨ì–´ê¸°ë°˜ RNN) : Link to generating text using a character-based RNN
* ë§í¬: https://www.tensorflow.org/text/tutorials/text_generation

## Quiz
1. What is the name of the method used to tokenize a list of sentences?
-> fit_on_texts(sentences)

1. If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, whatâ€™s the output shape?
-> (None, 116, 128)

1. What is the purpose of the embedding dimension?
-> It is the number of dimensions for the vector representing the word encoding

1. IMDB Reviews are either positive or negative. What type of loss function should be used in this scenario?
-> Binary crossentropy

1. If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?
-> Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace

1. When predicting words to generate poetry, the more words predicted the more likely it will end up gibberish. Why?
-> Because the probability that each word matches an existing phrase goes down the more words you create

1. What is a major drawback of word-based training for text generation instead of character-based generation?
-> Because there are far more words in a typical corpus than characters, it is much more memory intensive

1. How does an LSTM help understand meaning when words that qualify each other arenâ€™t necessarily beside each other in a sentence?
-> Values from earlier words can be carried to later ones via a cell state

## Assignment 
> Optional Assignment - Using LSTMs, see if you can write Shakespeare!    In this course youâ€™ve done a lot of NLP and text processing. This week you trained with a dataset of Irish songs to create traditional-sounding poetry. For this weekâ€™s exercise, youâ€™ll take a corpus of Shakespeare sonnets, and use them to train a model. Then, see if that model can create poetry!

* link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W4/assignment/C3_W4_Assignment.ipynb
* solution: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C3/C3/W4/assignment/C3_W4_Assignment_Solution.ipynb

## Wrap up - week 4.
> Over the last four weeks you've gotten a grounding in how to do Natural Language processing with TensorFlow and Keras. You went from first principles -- basic Tokenization and Padding of text to produce data structures that could be used in a Neural Network.   You then learned about embeddings, and how words could be mapped to vectors, and words of similar semantics given vectors pointing in a similar direction, giving you a mathematical model for their meaning, which could then be fed into a deep neural network for classification.   From there you started learning about sequence models, and how they help deepen your understanding of sentiment in text by not just looking at words in isolation, but also how their meanings change when they qualify one another.   You wrapped up by taking everything you learned and using it to build a poetry generator!   This is just a beginning in using TensorFlow for natural language processing. I hope it was a good start for you, and you feel equipped to go to the next level!

* í•´ì„: ì§€ë‚œ 4ì£¼ ë™ì•ˆ TensorFlow ë° Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ê¸°ì´ˆë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ì‹ ê²½ë§ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ í† í°í™” ë° íŒ¨ë”©ì´ë¼ëŠ” ì²« ë²ˆì§¸ ì›ì¹™ì—ì„œ ì¶œë°œí–ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ° ë‹¤ìŒ ì„ë² ë”©ì— ëŒ€í•´ ë°°ì› ê³  ë‹¨ì–´ê°€ ë²¡í„°ì— ë§¤í•‘ë˜ëŠ” ë°©ë²•ê³¼ ìœ ì‚¬í•œ ë°©í–¥ì„ ê°€ë¦¬í‚¤ëŠ” ë²¡í„°ê°€ ì œê³µë˜ëŠ” ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë¥¼ í†µí•´ ì˜ë¯¸ì— ëŒ€í•œ ìˆ˜í•™ì  ëª¨ë¸ì„ ì œê³µí•˜ê³  ë¶„ë¥˜ë¥¼ ìœ„í•´ ì‹¬ì¸µ ì‹ ê²½ë§ì— ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê±°ê¸°ì—ì„œ ì‹œí€€ìŠ¤ ëª¨ë¸ì— ëŒ€í•´ ë°°ìš°ê¸° ì‹œì‘í–ˆê³ , ë‹¨ì–´ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‚´í´ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë‹¨ì–´ê°€ ì„œë¡œë¥¼ ìˆ˜ì‹í•  ë•Œ ì˜ë¯¸ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‚´í´ë´„ìœ¼ë¡œì¨ í…ìŠ¤íŠ¸ì˜ ê°ì •ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¬í™”í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ë°©ë²•ì„ ë°°ìš°ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.

ë°°ìš´ ëª¨ë“  ê²ƒì„ ì‹œ ìƒì„±ê¸°ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ì‚¬ìš©í•˜ì—¬ ë§ˆë¬´ë¦¬í–ˆìŠµë‹ˆë‹¤!

ì´ê²ƒì€ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´ TensorFlowë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹œì‘ì— ë¶ˆê³¼í•©ë‹ˆë‹¤. ì¢‹ì€ ì‹œì‘ì´ ë˜ì—ˆê¸°ë¥¼ ë°”ë¼ë©° ë‹¤ìŒ ë‹¨ê³„ë¡œ ë‚˜ì•„ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤! 
