---
layout: single 
title:  "XGBoost(1) : 이해하기"
---
## XGBoost란?

"Extreme Gradient Boosting"을 의미한다. 

Boosting 기법을 이용해서 구현한 Gradient Boosting이 있고, 이 알고리즘을 병렬학습이 지원되도록 구현한 라이브러리가 XGBoost이다.

Regression, Classification 문제 모두 지원한다. 성능과 자원 효율이 좋다. 최근 Kaggle에서 아주 인기있다. 성능이 좋다보니 (상위 랭킹) 너도 나도 사용 중이라고 한다. *그러면 나도 사용해보자.*

> * Boosting이란? 여러 개의 약한 Decision Tree를 조합해서 사용하는 Ensemble 기법 중 하나이다. 약한 예측 모형들의 학습 에러에 가중치를 두고 순차적으로 다음 학습 모델에 반영하여 강한 예측 모형을 만든다.
> * Decision Tree(결정트리)란? 스무고개하듯 예/아니오를 이어가며 학습하는 지도학습 모델 중 하나. 특정 기준(질문)에 따라서 데이터를 구분하는 모델을 말한다. 한번 분기마다 변수 영역을 두 개로 구분한다. 결정트리에서 질문이나 정답을 담은 네모난 상자를 노드(node)라고 한다. 맨 처음 질문(분류기준)을 Root node라 하고, 맨 마지막 노드를 Terminal node 혹은 Leaf node라고 한다.

## 왜 XGBoost를 쓰는가?

- 효율성, 유연성, 휴대성이 뛰어나다.
- 유연한 학습 시스템: 파리미터들을 조절하면서 최적의 모델을 만들 수있다.
- 내부에 과적합 규제(Regularization)기능이 있다.
- 시각화가 쉽고 이해하기가 보다 직관적이다.
- CPU, 메모리가 많으면 많을수록 빠르게 학습하고 예측할 수 있다.
  - 병렬처리로 학습, 분류 속도가 빠르다.
- Cross Validation을 지원한다.
- 높은 성능을 나타낸다.
  - 분류와 회귀영역에서 뛰어난 예측 성능을 발휘한다고 한다.
  - Classification And Regression Tree (CART) 앙상블 모델 사용
- 수행속도가 빠르다.(C언어로 작성되어서)
- 조기종료(Early Stopping) 기능이 있다.
- 다양한 옵션을 제공하고 Customizing이 용이하다.
## 어떻게 학습되는가?
XGBoost 모델을 만드는 방법은 다양하다. 가장 보편적인 방법인 Regression을 먼저 알아보고, 다음으로 Classification도 알아보았다.
### 1. Regression(회귀)의 경우



### 2. Classification(분류)의 경우

## 하이퍼파라미터 튜닝

XGBoost의 많은 하이퍼파라메터는 세가지 범주로 나눌 수 있다. (이 [페이지](https://xgboost.readthedocs.io/en/latest/python/python_api.html)에서 참고함.)

### 일반 파라미터 :

* **부스팅 수행 시 트리를 사용할지, 선형 모델을 사용할지 선택**

* booster[default = gbtree]

  어떤 부스터 구조를 쓸지 결정

  의사결정기반모형(gbtree), 선형모형(gblinear), dart가 있음

* n_jobs

  XGBoost를 실행하는 데 사용되는 병렬 스레드 수

* verbosity [default = 1]

  0: 무음, 1: 경고, 2: 정보, 3: 디버그

### 부스터 파라미터

* **선택한 부스터에 따라서 적용할 수 있는 파라미터 종류가 다름**

#### gbtree Booster의 파라미터

* learning_rate [default = 0.3]

  높을 수록 `과적합`하기 쉬움

* n_estimators [default = 100]

  생성할 week learner의 수

  learning_rate이 낮은 값일 수록, n_estimators를 높여서 `과적합`을 방지함

* max_depth [default = 6]

  트리의 최대 깊이, 주로 3-10 사이의 값이 적용됨

  트리의 깊이가 깊을수록 모델이 복잡해지니까 `과적합`되기 쉬움

* min_child_weight [default = 1]

  관측치에 대한 가중치 합의 최소를 말함

  값이 높을수록 `과적합`이 방지됨

* gamma [default = 0]

  leaf node의 추가분할을 결정할 최소 손실 감소값

  해당값보다 손실이 크게 감소할 때 분리한다

  값이 높을수록 `과적합`이 방지됨

* subsample [default = 1]

  weak learner가 학습에 다용하는 데이터 샘플링 비율

  보통 0.5-1이 사용됨

  값이 낮을수록 `과적합`이 방지됨

* colsample_bytree [default = 1]

  각 트리별로 사용된 feature의 비율임

  보통 0.5-1이 사용됨

  값이 낮을수록 `과적합`이 방지됨

* lambda [default = 1, 별칭: reg_lambda]

  가중치에 대한 L2 Regularization 적용 값

  feature 갯수가 많을 때 적용을 검토한다

  이 값이 클수록 `과적합`이 감소되는 효과가 있다

* alpha [default = 0,  별칭: reg_alpha]

  가중치에 대한 L1 Regularization 적용 값

  feature 갯수가 많을 때 적용을 검토한다

  이 값이 클수록 `과적합`이 감소되는 효과가 있다

### 학습 과정 파라미터

- **objective [ default : reg = squarederror ]**
  - reg : squarederror
    - 제곱 손실이 있는 회귀
  - binary : logistic (binary-logistic classification)
    - 이항 분류 문제 로지스틱 회귀 모형으로 반환값이 클래스가 아니라 예측 확률
  - **multi : softmax**
    - 다항 분류 문제의 경우 소프트맥스(Softmax)를 사용해서 분류하는데 반환되는 값이 예측확률이 아니라 클래스이다. 또한 num_class도 지정해야함.
  - **multi : softprob**
    - 각 클래스 범주에 속하는 예측확률을 반환함.
  - count : poisson (count data poison regression) 등 다양함.
- eval_metric
  - 모델의 평가 함수를 조정하는 함수
  - 설정한 objective 별로 기본설정값이 지정되어 있음
  - rmse: root mean square error
  - mae: mean absolute error
  - logloss: negative log-likelihood
  - error: Binary classification error rate (0.5 threshold)
  - **merror: Multiclass classification error rate**
  - **mlogloss: Multiclass logloss**
  - auc: Area under the curve
  - map (mean average precision)등, 해당 데이터의 특성에 맞게 평가 함수를 조정
- seed [ default  = 0 ]
  - 난수 고정

### 프로젝트의 성격에 따라 민감하게 조정해야하는 하이퍼파라메터들

- booster 모양
- objective(목적함수)
- eval_metric(평가함수)
- eta
- L1 모양 *(참고: L1이 L2보다 이상치(outlier)에 민감함)*
- L2 모양

### 과적합 방지를 위해 조정해야하는 파라메터들 정리

- learning_rate는 낮추기 🔽 →  n_estimators는 높이기🔺
- max_depth는 낮추기 🔽
- min_child_weight은 높이기🔺
- gamma는 높이기🔺
- subsample(보통 0.5~1), colsample_bytree(보통 0.5~1)는 낮추기 🔽

## 다음 글로 이동

스스로 공부하기 위해 이 블로그 저 블로그를 보며 일단 기본 개념을 정리했습니다.

시계열데이터를 이용한 Multi Classification에 xgboost를 이용해보려고 하고 있어서, 저에게 특히 필요한 부분들에 **하이라이팅**이 되어있습니다. 특별한 의미는 없으니 참고하시기 바랍니다. 

참고한 블로그, 홈페이지들은 다음과 같습니다.

> https://xgboost.readthedocs.io/en/latest/python/python_api.html
>
> https://dining-developer.tistory.com/4?category=929228
>
> https://wooono.tistory.com/97

[이 링크](https://nkim505.github.io/xgboost2/)를 클릭해서 다음 포스팅인 XGBoost(2)로 이동하세요!
