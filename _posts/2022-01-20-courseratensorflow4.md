---
layout: single #postë„ ê°€ëŠ¥
title:  "Deeplearning.Ai TensorFlow Developer (Course 4)"
---

**<<ì•Œë¦¼>>**<br>
ğŸ’â€â™€ï¸ í…ì„œí”Œë¡œìš° ìê²©ì¦ ì·¨ë“ì— ë„ì›€ì´ ë˜ëŠ” **ì½”ì„¸ë¼** ê°•ì˜ <br>
ğŸ’» ["Deeplearning.Ai TensorFlow Developer"](https://www.coursera.org/professional-certificates/tensorflow-in-practice?trk_ref=globalnav) - Course 4 : Sequences, Time Series and Predictionì„ ë“£ê³  ê°•ì˜ ë‚´ìš©ì„ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.<br>
ğŸ§  ìˆ˜ì—…ì„ ë“¤ìœ¼ë©° ë™ì‹œì— ì •ë¦¬í•œ ë‚´ìš©ì´ì–´ì„œ(í•„ê¸°ë…¸íŠ¸ ëŒ€ìš©), ì˜ì‹ì˜ íë¦„ì´ ê°•í•˜ê²Œ ê°œì…ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
ğŸ˜š ì €ë§Œì˜ ì´í•´ ë°©ë²•ì„ í’€ì–´ ë†“ì•„, ê°•ì˜ì™€ í•¨ê»˜ ë³´ì‹œëŠ” ë¶„ê»˜ëŠ” ì‘ì€ ë„ì›€ì´ ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.<br>



# Week 1. Sequences and Prediction
## Lectures
### Train, validation and test sets
* Fixed partitioning: We typically want to split the time series into a training period, a validation period and a test period.

- Here's where you can experiment to find the right architecture for training. And work on it and your hyper parameters, until you get the desired performance, measured using the validation set. Often, once you've done that, you can retrain using both the training and validation data. And then test on the test period to see if your model will perform just as well. And if it does, then you could take the unusual step of retraining again, using also the test data. But why would you do that? Well, it's because the test data is the closest data you have to the current point in time. And as such it's often the strongest signal in determining future values. If your model is not trained using that data, too, then it may not be optimal. 

* roll-forward partitioning: We start with a short training period, and we gradually increase it, say by one day at a time, or by one week at a time. At each iteration, we train the model on a training period. And we use it to forecast the following day, or the following week, in the validation period. 

### Metrics for evaluating performance

```python
errors = forecasts - actual
mse = np.square(errors).mean() # mean squared error
rmse = np.sqrt(mse) # root mean squared error
mae = np.abs(error).mean() # mean absolute error / also called the main absolute deviation or mad / ì ˆëŒ€ê°’ì´ í° ê°’ì—ê²Œ ê·¸ë ‡ê²Œ ë§ì€ í˜ë„ë¼ì´ì¦ˆë¥¼ ë¶€ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤(ë§ˆì¹˜ mean squared error ê°€ í•˜ëŠ” ê²ƒ ì²˜ëŸ¼)
mape = # mean absolute percentage error  / mean ratio between ì—ëŸ¬ì˜ ì ˆëŒ€ê°’ and valueì˜ ì ˆëŒ€ê°’

```
* ì—…ë¬´ì— ë”°ë¼ì„œ MAEë‚˜ MSEë¥¼ ì“°ëŠ” ê²ƒì„ ì„ í˜¸í•  ìˆ˜ë„ ìˆë‹¤.
- ì˜ˆë¥¼ ë“¤ì–´ì„œ í° ì—ëŸ¬ê°€ ìœ„í—˜í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  í•˜ê³  ê·¸ëŸ° í° ì—ëŸ¬ë“¤ì´ costí•˜ê²Œ ë˜ë©´ MSEë¥¼ ì“°ëŠ” ê²Œ ì„ í˜¸ë  ìˆ˜ ìˆë‹¤.
- í•˜ì§€ë§Œ ë‚˜ì˜ gainê³¼ lossê°€ ì—ëŸ¬ ì‚¬ì´ì¦ˆì— ë‹¨ì§€ ë¹„ë¡€í•œë‹¤ê³  í•˜ë©´, MAEê°€ ë‚«ë‹¤.
* mape: valueë“¤ê³¼ ë¹„êµí•œ ì—ëŸ¬ì˜ ì‚¬ì´ì¦ˆì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¤€ë‹¤. (this gives an idea of the size of the errors compared to the values.)

### Moving average and differencing

* ì´ë™ í‰ê· (MA) : ì„¤ì •í•œ Averaging Window (ì˜ˆë¥¼ ë“¤ì–´ 30ì¼, 1ë…„) ì•ˆì—ì„œ í‰ê· ê°’ì„ ì·¨í•œë‹¤. 

* ì°¨ë¶„: series(t) - series(t-365) í•´ì„œ ì¶”ì„¸ì™€ íŠ¸ë Œë“œë¥¼ ì‚¬ë¼ì§€ê²Œ ë§Œë“¬. ì´í›„ì— Moving averageë¥¼ êµ¬í•¨. ì´ê²ƒì€ ì˜¤ë¦¬ì§€ë„ ì‹œê³„ì—´ ë°ì´í„°ì˜ MAê°€ ì•„ë‹Œ ì°¨ë¶„ëœ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì´ë™í‰ê· ì„.
* To get the final forecasts for the original time series, we just need to add back the value at time T minus 365 : ì˜¤ë¦¬ì§€ë„ ì‹œê³„ì—´ ë°ì´í„° ì¶”ì •ì„ í•˜ê¸° ìœ„í•´ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ A_(t-365) ê°’ì„ ë‹¤ì‹œ ë”í•´ì¤˜ì•¼í•œë‹¤.
* ** Forcast = moving average of defferenced series + series(t-365)**

* ë‹¤ì‹œ ëºë˜ ê°’ì„ ë„£ì–´ì£¼ë‹ˆê¹Œ ì–´ëŠ ì •ë„ ë…¸ì´ì¦ˆëŠ” ë‹¤ì‹œ ìƒê²¼ë‹¤. ê·¸ëŸ¬ë©´,   Where does that noise come from?   Well, that's coming from the past values that we added back into our forecasts
-->  we can improve these forecasts by also removing the past noise using a moving average on that. If we do that, we get much smoother forecasts. 
* ì´ ì˜ˆì¸¡ì„ ë” ë‚«ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œ ê³¼ê±° ë…¸ì´ì¦ˆë¥¼ ì§€ì›€ìœ¼ë¡œì¨ ë” ìŠ¤ë¬´ìŠ¤í•œ ì˜ˆì¸¡ì„ ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤.

* ** Forcast = trailing MA of differenced series + centered MA of past series(t-365)**

>  In fact, since the series is generated, we can compute that a perfect model will give a mean absolute error of about four due to the noise. Apparently, with this approach, we're not too far from the optimal. Keep this in mind before you rush into deep learning. Simple approaches sometimes can work just fine.

### Trailing versus centered windows (í›„í–‰ì°½ê³¼ ì¤‘ì•™ì°½?)
> Note that when we use the trailing window when computing the moving average of present values from t minus 32, t minus one. But when we use a centered window to compute the moving average of past values from one year ago, that's t minus one year minus five days, to t minus one year plus five days. Then moving averages using centered windows can be more accurate than using trailing windows. But we can't use centered windows to smooth present values since we don't know future values. However, to smooth past values we can afford to use centered windows. 


## Quiz
1. What is an example of a Univariate time series?    hour by hour temperature
1. What is an example of a Multivariate time series?    Hour by hour weather 
1. What is imputed data?    A projection of unknown (usually past or missing) data
1. A sound wave is a good example of time series data    T
1. What is Seasonality?    A regular change in shape of the data
1. What is a trend?    An overall direction for data regardless of direction
1. In the context of time series, what is noise?    
1. What is autocorrelation?    Data that follows a predictable shape, even if the scale is different
1. What is a non-stationary time series?    One that has a disruptive event breaking trend and seasonality 

## Optional Assignment - Create and predict synthetic data
* link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C4/C4/W1/assignment/C4_W1_Assignment.ipynb
* solution: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C4/C4/W1/assignment/C4_W1_Assignment_Solution.ipynb

# Week 2. Deep Neural Network for Time Series
## Lectures
### Sequence bias
> Sequence bias is when the order of things can impact the selection of things. For example, if I were to ask you your favorite TV show, and listed "Game of Thrones", "Killing Eve", "Travellers" and "Doctor Who" in that order, you're probably more likely to select 'Game of Thrones' as you are familiar with it, and it's the first thing you see. Even if it is equal to the other TV shows. So, when training data in a dataset, we don't want the sequence to impact the training in a similar way, so it's good to shuffle them up. 
í•œêµ­ì–´ë¡œ, ì‹œí€€ìŠ¤ í¸í–¥??? ì¸ê°€

## Quiz
1.
ì§ˆë¬¸ 1
What is a windowed dataset?
- A fixed-size subset of a time series.

2. What does â€˜drop_remainder=trueâ€™ do?
- It ensures that all rows in the data window are the same length by cropping data

3. Whatâ€™s the correct line of code to split an n column window into n-1 columns for features and 1 column for a label
- dataset = dataset.map(lambda window: (window[:-1], window[-1:]))


4. What does MSE stand for?
- Mean Squared error
5. What does MAE stand for?
- Mean Absolute Error
6. If time values are in time[], series values are in series[] and we want to split the series into training and validation at time 1000, what is the correct code?

time_train = time[:split_time]

x_train = series[:split_time]

time_valid = time[split_time:]

x_valid = series[split_time:]
7. If you want to inspect the learned parameters in a layer after training, whatâ€™s a good technique to use?
- Assign a variable to the layer and add it to the model using that variable. Inspect its properties after training
8. How do you set the learning rate of the SGD optimizer? 
- Use the lr property
9. If you want to amend the learning rate of the optimizer on the fly, after each epoch, what do you do?
- Use a LearningRateScheduler object in the callbacks namespace and assign that to the callback 

## Wrap up
> You've now explored time series data, and seen how to split it into training and validation sets for training a DNN.
> But sequence data tends to work better with RNNs, so next week you're going to look at training a model using RNNs and LSTMs on this data to see what happens!

## Optional Assignment - Predict with a DNN (í•´ë³´ê¸°)
> In class you saw how to split a dataset, and how to start training a DNN using it. For this exercise youâ€™ll create your own synthetic dataset -- Iâ€™ve plotted a chart for what it should look like, see if you can figure out the parameters that get this series.   Once you have your series, youâ€™ll create a DNN to predict values for that series

* link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C4/C4/W2/assignment/C4_W2_Assignment.ipynb
* solution: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C4/C4/W2/assignment/C4_W2_Assignment_Solution.ipynb

# Week 3. Recurrent Neural Networks for Time Series
## Lectures
### LSTM lecture link
* https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay


## Quiz
1. If X is the standard notation for the input to an RNN, what are the standard notations for the outputs?
* Y(hat) and H
1. What is a sequence to vector if an RNN has 30 cells numbered 0 to 29
* The Y(hat) for the last cell
1. What does a Lambda layer in a neural network do?
* Allows you to execute arbitrary code while training
1. What does the axis parameter of tf.expand_dims do?
* Defines the dimension index at which you will expand the shape of the tensor 
1. A new loss function was introduced in this module, named after a famous statistician. What is it called?
* Huber loss
1. Whatâ€™s the primary difference between a simple RNN and an LSTM
* In addition to the H output, LSTMs have a cell state that runs across all cells 
1. If you want to clear out all temporary variables that tensorflow might have from previous sessions, what code do you run?
* tf.keras.backend.clear_session()  
1. What happens if you define a neural network with these two layers?
* Your model will fail because you need return_sequences=True after the first LSTM layer

## Wrap up
> Now that you've built on your DNNs with RNNs and LSTMs, it's time to put it all together using CNNs and some real world data.  Next week you'll do that, exploring prediction of sunspot data from a couple of hundred years worth of measurements!
RNNê³¼ LSTMìœ¼ë¡œ DNNì„ êµ¬ì¶•í–ˆìœ¼ë¯¸ë¡œ ì´ì œ CNNê³¼ ì‹¤ì œ ë°ì´í„°ë¡œ ëª¨ë“  ê²ƒì„ í†µí•©í•  ê²ƒì´ë‹¤.   ë‹¤ìŒ ì£¼ì—ì„œëŠ” í‘ì  ë°ì´í„°ë¡œ ì˜ˆì¸¡í•´ë³´ë©´ì„œ ë°°ìš°ê² ë‹¤.

## Assignment -Mean Absolute Error (í•´ë³´ê¸°)
> In class you learned about RNNs and LSTMs for prediction, as well as a simple methodology to pick a decent learning rate for the stochastic gradient descent optimizer. In this exercise youâ€™ll take a synthetic data set and write the code to pick the learning rate and then train on it to get an MAE of < 3

* link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/25_august_2021_fixes/C4/W3/assignment/C4_W3_Assignment.ipynb
* solution: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/25_august_2021_fixes/C4/W3/assignment/C4_W3_Assignment_Solution.ipynb

# Week 4. Real-world time series data
## Lectures
### Convolutional neural networks course
* link: https://www.coursera.org/learn/convolutional-neural-networks/home/welcome
### More on batch sizing (Mini Batch Gradient Descent(C2W2L01))
* link: https://www.youtube.com/watch?v=4qJaSmvhxi8
### lab1. LSTM
* link: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C4/W4/ungraded_labs/C4_W4_Lab_1_LSTM.ipynb
### Train and tune the model
#### ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³  ë‹¤ì‹œ ì¡°ì •í•˜ë©´ì„œ ê±°ì¹˜ëŠ” ê³¼ì •
* `window_size ë³€ê²½`: 20->132 ì£¼ê¸°ê°€ 11ë…„ ì •ë„ ë˜ì–´ë³´ì—¬ì„œ ì£¼ê¸°ë¥¼ ì»¤ë²„í•˜ëŠ” ì •ë„ë¡œ ìœˆë„ìš° ì‚¬ì´ì¦ˆë¥¼ ë³€ê²½í•´ ë³¸ë‹¤.   <br> -> ê²°ê³¼: MAE ë” ë‚˜ë¹ ì§. íš¨ê³¼ ì—†ì—ˆë‹¤. <br> -> ì´ìœ ? : ìœˆë„ìš°ì— ì‹œì¦Œ í•˜ë‚˜ë¥¼ ê½‰ ì±„ì›Œì„œ ë„£ì„ í•„ìš”ëŠ” ì—†ë‹¤. (we dont need a full season in our window). ë°ì´í„° ê¸°ê°„ì„ ë” í™•ëŒ€í•´ì„œ ë³´ë©´ ì•ˆì—ë„ ì‹œì¦Œ 1ê°œ ì•ˆì—ë„ ë¬´ìˆ˜íˆ ë§ì€ ë…¸ì´ì¦ˆê°€ ìˆê³ , ì´ ë…¸ì´ì¦ˆì˜ ê¸¸ì´ê°€ í•œ 20 ì •ë„ ë˜ì–´ ë³´ì„
* `window_size ë³€ê²½2` + `split_time ëŠ˜ì„` : 20 -> 30, 1000 -> 3500. ë³´ë‹ˆê¹Œ train ë°ì´í„°ê°€ 1000 ì´ê³  validationì´ ë” ë§ì•„ì„œ ì¶©ë¶„í•œ train data ê°€ ì—†ì—ˆë˜ ê²ƒ ê°™ë‹¤.  <br>  -> ê²°ê³¼: MAEê°€ ë” ì‘ê²Œ ë‚˜ì™€ì„œ íš¨ê³¼ê°€ ìˆì—ˆë‹¤.  <br>  -> ë” í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œ?
* `ë‰´ëŸ°ë„¤íŠ¸ì›Œí¬ ë””ìì¸ì„ ìˆ˜ì •`: 3 layers of 10, 10, and 1 neurons -> 30, 15, 1   <br> ->  ê²°ê³¼: ì•½ê°„ ì´ ì „ì˜ ìƒí™©ìœ¼ë¡œ ëŒì•„ê°”ë‹¤. MAEê°€ ë„ë¦¬ì–´ ì˜¬ë¼ê°.   <br> -> ë‹¤ì‹œ 10, 10, 1ë¡œ ëŒë ¤ë†“ìŒ
* `learning_rate ìˆ˜ì •(tweak tweak tweak~)`: MAE getting a little smaller

### Prediction
### Lab 2, lab 3
* Sunspot notebook : https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/adding_C4/C4/W4/ungraded_labs/C4_W4_Lab_2_Sunspots.ipynb#scrollTo=PrktQX3hKYex
*  a version of the notebook that uses only DNN: https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C4/W4/ungraded_labs/C4_W4_Lab_3_DNN_only.ipynb

## Quiz
1. How do you add a 1 dimensional convolution to your model for predicting time series data?
* Use a Conv1D layer type
1. Whatâ€™s the input shape for a univariate time series to a Conv1D? 
* [None, 1]
1. You used a sunspots dataset that was stored in CSV. Whatâ€™s the name of the Python library used to read CSVs?
* CSV
1. If your CSV file has a header that you donâ€™t want to read into your dataset, what do you execute before iterating through the file using a â€˜readerâ€™ object?
* next(reader)
1. When you read a row from a reader and want to cast column 2 to another data type, for example, a float, whatâ€™s the correct syntax?
* float(row[2]) 
1. What was the sunspot seasonality?
* **11 or 22 years depending on who you ask**
1. After studying this course, what neural network type do you think is best for predicting time series like our sunspots dataset?
* A combination of all of the above
1. Why is MAE a good analytic for measuring accuracy of predictions for time series?
* It doesnâ€™t heavily punish larger errors like square errors do

## Optional Assignment - Sunspots
> This week you moved away from synthetic data to do a real-world prediction -- sunspots. You loaded data from CSV and built models to use it. For this weekâ€™s exercise, youâ€™ll use a dataset from Jason Brownlee, author of the amazing MachineLearningMastery.com site and who has shared lots of datasets at https://github.com/jbrownlee/Datasets. Itâ€™s a dataset of daily minimum temperatures in the city of Melbourne, Australia measured from 1981 to 1990.  Your task is to download the dataset, parse the CSV, create a time series and build a prediction model from it. Your model should have an MAE of less than 2, and as you can see in the output, mine had 1.78. Iâ€™m sure you can beat that! :)

* link: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/25_august_2021_fixes/C4/W4/assignment/C4_W4_Assignment.ipynb
* solution: https://colab.sandbox.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/25_august_2021_fixes/C4/W4/assignment/C4_W4_Assignment_Solution.ipynb


