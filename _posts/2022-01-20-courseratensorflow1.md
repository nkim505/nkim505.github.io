---
layout: single #postë„ ê°€ëŠ¥
title:  "Deeplearning.Ai TensorFlow Developer (Course 1)"
---

** [ì•Œë¦¼] **  <br>
ğŸ’â€â™€ï¸ í…ì„œí”Œë¡œìš° ìê²©ì¦ ì·¨ë“ì— ë„ì›€ì´ ë˜ëŠ” **ì½”ì„¸ë¼** ê°•ì˜ <br>
ğŸ’» ["Deeplearning.Ai TensorFlow Developer"](https://www.coursera.org/professional-certificates/tensorflow-in-practice?trk_ref=globalnav) Course 1 : Introductionì„ ë“£ê³  ê°•ì˜ ë‚´ìš©ì„ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.<br>
ğŸ§  ìˆ˜ì—…ì„ ë“¤ìœ¼ë©° ë™ì‹œì— ì •ë¦¬í•œ ë‚´ìš©ì´ì–´ì„œ(í•„ê¸°ë…¸íŠ¸ ëŒ€ìš©), ì˜ì‹ì˜ íë¦„ì´ ê°•í•˜ê²Œ ê°œì…ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
ğŸ˜š ì €ë§Œì˜ ì´í•´ ë°©ë²•ì„ í’€ì–´ ë†“ì•„, ê°•ì˜ì™€ í•¨ê»˜ ë³´ì‹œëŠ” ë¶„ê»˜ëŠ” ì‘ì€ ë„ì›€ì´ ë  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.<br>

# Week 2.
## Lectures
```python
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
```
<br>

>Using a number is a first step in avoiding bias -- instead of labeling it with words in a specific language and excluding people who donâ€™t speak that language! You can learn more about bias and techniques to avoid it [here.](https://ai.google/responsibilities/responsible-ai-practices/)
<br>

```python
model = keras.Sequantial([
	keras.layers.Flatten(input_shape = (28, 28),
	keras.layers.Dense(128, activation=tf.nn.relu),
	keras.layers.Dense(10, activation = tf.nn.softmax)
])
```

`keras.layers.Dense(units=neuronê°¯ìˆ˜, activation =, input_shape=)` í•˜ë‚˜ê°€ layer í•˜ë‚˜ì˜ ì„¤ì • ë§í•œë‹¤.   
`keras.layers.Flatten`ë„ ë ˆì´ì–´ í•˜ë‚˜ë¥¼ ëœ»í•œë‹¤.   
ë”°ë¼ì„œ ìœ„ëŠ” three layersì´ë‹¤.   
ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” 10ê°œ ë‰´ë¡ ì´ë‹¤. ì™œëƒë©´ 10ê°œì˜ ì˜· ë°ì´í„°ì…‹ì´ê¸°ë•Œë¬¸ì—. ì´ê±´ ì–¸ì œë‚˜ ë§ì¶°ì¤˜ì•¼í•œë‹¤.   
ì²« ë ˆì´ì–´ëŠ” input layerì¸ë° ì¸í’‹ì…°ì…ì´ 28,28 ì„ì„ ì•Œë ¤ì£¼ëŠ” optionì´ ë“¤ì–´ê°€ìˆë‹¤. ì´ê±¸ flatten 1ê°œì˜ arrayë¡œ ë°”ê¿”ì„œ ë„£ì–´ì¤Œ.


### Hello World. A Computer Vision Example   
 

```python
import tensorflow as tf
print(tf.__version__)

mnist = tf.keras.datasets.fashion_mnist

(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
```

```python
import matplotlib.pyplot as plt
plt.imghow(training_images[0])
print(training_labels[0])
print(training_images[0])

#normalizing
training_images = training_images/255.0
test_images = test_images/255.0
```

```python
#design the model
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
tf.keras.layers.Dense(128, activation=tf.nn.relu),
tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

#complie the model to find the loss function and the optimizer
model.compile(optimizer = tf.train.AdamOptimizer(), loss = 'sparse_categorical_crossentropy')

#fit the model 
model.fit(training_images, trainin_labels, epochs=5)
```
+ `compiling model`: the goal is to make a guess as to what the relationship is btw the input data and the output data   
and measure how well or how badly it did using the loss function, use the `optimizer` to generate a new guess and repeat.   
+ `fitting model`: trying to fit the training images to the training labels.   

ê·¸ë¦¬ê³  Test data ë¡œ ë„˜ì–´ê°„ë‹¤. ì—¬ê¸°ì„œ loss ê°’ì´ ë” ë‚˜ì˜ê²Œ ë‚˜ì˜¤ë©´ ë¬¸ì œê°€ ìˆë‹¤. test setì—ì„œ ëœ ì •í™•í•œ ê²ƒ.   
ì´ë•Œ ì‹ ê²½ë§ì˜ íŒŒë¼ë©”í„°ë¥¼ ë°”ê¾¸ê±°ë‚˜ epochsë¥¼ ë°”ê¾¸ê±°ë‚˜ ë°©ë²•ì„ ì‹œë„í•  ìˆ˜ ìˆë‹¤. [ì—¬ê¸°ì„œ('Beyond Hello World, A Computer Vision Example')](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W2/ungraded_labs/C1_W2_Lab_1_beyond_hello_world.ipynb) í•´ë‹¹ ë…¸íŠ¸ë¶ì„ ì¨ì„œ í•´ë³¼ ìˆ˜ ìˆë‹¤.   

>Your job now is to go through the workbook, try the exercises and see by tweaking the parameters on the neural network or changing the epochs   
   

### Using callbacks to control training
> Q. How can I stop training when I reach a point that I want to be at? What do I always have to hard code it to go for a certain number of epochs?    A. The training loop does support callbacks

ëª¨ë“  epochë§ˆë‹¤ ìš°ë¦¬ëŠ” ì½”ë“œ í•¨ìˆ˜ë¥¼ callback í•  ìˆ˜ ìˆë‹¤.

```python
model.fit(training_images, training_labels, epochs = 5
```
ì—¬ê¸°ì„œ model.fit í•¨ìˆ˜ëŠ” íŠ¸ë ˆì´ë‹ ë£¨í”„ë¥¼ ì‹¤í–‰í•œë‹¤.

ê·¸ë¦¬ê³  callback ì½”ë“œë¥¼ íŒŒì´ì¬ìœ¼ë¡œ ì‘ì„±í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

```python
class myCallback(tf.keras.callbacks.Callback):
def on_epoch_end(self,epoch, logs={}):
if logs.get('loss')<0.4:
print("\n Loss is low so cancelling training!")
self.model.stop_training = True
```

ì´ì œ ìš°ë¦¬ê°€ ë§Œë“  myCallback í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ì›ë˜ ë§Œë“¤ì–´ë†¨ë˜ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©´ ì´ë ‡ê²Œ ëœë‹¤.

```python
callbacks = myCallback()
mnist = tf.keras.datasets.fashion_mnist
(training_images, traning_labels), (test_images, test_labels) = mnist.load_data()
training_images = training_images/255.0
test_images = test_images / 255.0
model = tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape - (28,28)),
tf.keras.layers.Dense(512, activation = tf.nn.relu),
tf.keras.layers.Dense(10, activation = tf.nn.softmax)
])
model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')
model.fit(training_images, training_labels, epochs = 5)
```
<br>
ì‹œì‘ì ì—ì„œ `callback = myCallback()` ë„£ì–´ì£¼ê³ , ë§ˆì§€ë§‰ì— model.fit ì•ˆì—ì„œ íŒŒë¼ë©”í„°ë¡œ `callbacks = [callbacks]`ì„ ë„£ì–´ì¤€ë‹¤.
<br>
<br>
> See how to implement Callbacks (Lab 2)   Experiment with using Callbacks in [this notebook](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W2/ungraded_labs/C1_W2_Lab_2_callbacks.ipynb) -- work through it to see how they perform!
<br>

```python
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.6):
      print("\nReached 60% accuracy so cancelling training!")
      self.model.stop_training = True

mnist = tf.keras.datasets.fashion_mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

callbacks = myCallback()

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer=tf.optimizers.Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])
```
<br>
<br>

**exercise tip** : nomalizationí•˜ë‹ˆê¹Œ 5ë²ˆì§¸ epochì—ì„œ acc>0.99 ì„±ê³µí•¨.
nomalization ì•ˆí–ˆì„ ë•ŒëŠ” 10ë²ˆì§¸ì—ì„œë„ 0.95 ì •ë„ì˜€ìŒ.




# Week 3. <br> Enhancing Vision with Convolutional Neural Networks
## Lectures
### What are convolutions and pooling?

>  there's a lot of wasted space in each image. While there are only 784 pixels, it will be interesting to see if there was a way that we could condense the image down to the important features that distinguish what makes it a shoe, or a handbag, or a shirt. That's where convolutions come in

convolutionê³¼ poolingì€ ì´ë¯¸ Andrew ê°•ì˜ì—ì„œ ê³µë¶€í–ˆìœ¼ë¯€ë¡œ ìì„¸í•œ ë…¸íŠ¸í•˜ì§€ ì•ŠìŒ.
Convolutionê³¼ poolingì„ í•¨ê»˜ ì“°ë©´ ë§¤ìš° powerfulí•´ì§„ë‹¤.
Convolutionì€ ` they narrow down the content of the image to focus on specific, distinct, details.`í•˜ëŠ” ê²ƒì´ë‹¤.

> * [Conv2D ì„¤ëª…ì‚¬ì´íŠ¸](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)   * [MaxPooling2D ì„¤ëª… ì‚¬ì´íŠ¸](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) 

### Implementing Convolutional layers (ì½”ë”©ìœ¼ë¡œ êµ¬í˜„í•˜ê¸°)

```puthon
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
```

ê¸°ë³¸ì ìœ¼ë¡œ ìœ„ì˜ ì½”ë“œì—ì„œ ì•„ë˜ë¡œ ë°”ë€Œê²Œ ëœë‹¤.


```puthon
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
```

* `tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1))` : 64ê°œì˜ í•„í„°ë¥¼ ë§Œë“ ë‹¤. í•„í„°ë“¤ì€  3x3 ì´ê³  í™œì„±í•¨ìˆ˜ëŠ” relu (ì¦‰, ìŒìˆ˜ ê°’ì€ ëª¨ë‘ ë‚ ë ¤ë²„ë¦¬ê²Œ). input_shapeì€ 28 x 28 ì´ë‹¤.  28 x 28 x 1 ì„ ì“´ ì´ìœ ëŠ”  color depthë¡œ single byteë¥¼ ì“´ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. (ì´ ë°ì´í„°ëŠ” íšŒìƒ‰scaleì„)
* `64ê°œì˜ í•„í„°` : 'you might wonder what the 64 filters are. It's a little beyond the scope of this class to define them, but they aren't random. They start with a set of known good filters in a similar way to the pattern fitting that you saw earlier, and the ones that work from that set are learned over time.' ì´ í•„í„°ë“¤ì€ ë¬´ì‘ìœ„ëŠ” ì•„ë‹ˆê³  ì¢‹ì€ í•„í„°ë“¤ì˜ ì„¸íŠ¸ë¡œ ì‹œì‘í•´ì„œ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë ˆ ê³„ì† í•™ìŠµë¨.
* ì»¨ë³¼ë£¨ì…˜ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ : [http://bit.ly/2UGa7uH](http://bit.ly/2UGa7uH)


### Implementing pooling layers
* `tf.keras.layers.MaxPooling2D(2,2)` : 2x2poolë¡œ ëª¨ë“  4ê°œì˜ í”½ì…€ë§ˆë‹¤ ê°€ì¥ í° 1ê°œì˜ í”½ì…€ë§Œ ì·¨í•˜ê²Œ ëœë‹¤. 

*`tf.keras.layers.Conv2D(64, (3,3), activation='relu'),   
  tf.keras.layers.MaxPooling2D(2,2)` : ê·¸ë¦¬ê³  ë‹¤ë¥¸ Cov ë ˆì´ì–´ì™€ Pool ë ˆì´ì–´ê°€ ë˜ ë”°ë¼ì˜¨ë‹¤. ê·¸ë˜ì„œ ë‹¤ë¥¸ ì»¨ë¸Œ í•™ìŠµ í•„í„°ë¥¼ ê±°ì¹˜ê³  í’€ë§ì„ í†µí•´ ì‚¬ì´ì¦ˆê°€ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.

ê·¸ë˜ì„œ ì´ë¯¸ Dense ë ˆì´ì–´ë¡œ ë„˜ì–´ê°ˆ ë•Œ ë°ì´í„°ëŠ” ì•„ì£¼ì•„ì£¼ simplified ëœë‹¤.

```python
model.summary()
```
: ëª¨ë¸ì˜ ë ˆì´ì–´ë“¤ì„ ì ê²€í•˜ê²Œ í•´ì¤€ë‹¤. ê·¸ë¦¬ê³  ì»¨ë³¼ë£¨ì…˜í•˜ëŠ” ì´ë¯¸ì§€ë“¤ì˜ ì—¬ì •ì„ ê²°ê³¼ ëª¨ì–‘ê³¼ í•¨ê»˜ ì§ì ‘ ëˆˆìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. It's important to keep an eye on the output shape column. When you first look at this, it can be a little bit confusing and feel like a bug. After all, isn't the data 28 by 28, so y is the output, 26 by 26. --> ê°€ì¥ ê°€ì¥ìë¦¬ì˜ í”½ì…€ë“¤ì€ 3x3ì„ ì“¸ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë‚ ì•„ê°„ë‹¤. ê·¸ë ‡ê²Œ ìœ„ ì•„ë˜ ì˜†ìœ¼ë¡œ í•œ ì¤„ì”© ëª»ì“°ê³  ì—†ì–´ì§€ê¸° ë•Œë¬¸ì— 28 by 28ì´ ì•„ë‹ˆë¼ 26 by 26ì´ ëœë‹¤.


### Improving the Fashion classifier with convolutions

### Try it for yourself (Lab 1)   
>[Here](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W3/ungraded_labs/C1_W3_Lab_1_improving_accuracy_using_convolutions.ipynb)â€™s the notebook that Laurence was using in that screencast. To make it work quicker, go to the â€˜Runtimeâ€™ menu, and select â€˜Change runtime typeâ€™. Then select GPU as the hardware accelerator! <br> Work through it, and try some of the exercises at the bottom! It's really worth spending a bit of time on these because, as before, they'll really help you by seeing the impact of small changes to various parameters in the code. You should spend at least 1 hour on this today!  <br>   Once youâ€™re done, go onto the next video and take a look at some code to build a convolution yourself to visualize how it works!   

### Visualizing the Convolutions and Pooling
```python
print(test_labels[:100]
import matplotlib.pyplot as plt
f, axarr = plt.subplots(3,4)
FIRST_IMAGE=0
SECOND_IMAGE=7
THIRD_IMAGE=26
CONVOLUTION_NUMBER = 1
from tensorflow.keras import models
layer_outputs = [layer.output for layer in model.layers]
activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)
for x in range(0,4):
  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[0,x].grid(False)
  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[1,x].grid(False)
  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[2,x].grid(False)
```

**here are some exercises to consider by tweaking codes**

1. Try editing the convolutions(below). Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.
--> accuracyëŠ” 16ì—ì„œ 0.9825, 64ì—ì„œ 0.9850ìœ¼ë¡œ ì˜¬ë¼ê°”ë‹¤. fillterê°€ 32ì¼ ë•Œ ì œì¼ ë†’ê²Œ ë‚˜ì™”ë‹¤. ê³„ì‚°ì‹œê°„ì€ 16ì—ì„œê°€ ì œì¼ ë¹ ë¥¼ ê²ƒ ê°™ì€ë°, ì •í™•íˆ ì¬ë³¸ ê²ƒì´ ì•„ë‹ˆë¼ì„œ ëª¨ë¥´ê² ë‹¤.
2. Remove the final Convolution. What impact will this have on accuracy or training time?
--> Convolutionì´ 2ê°œì¼ ë•ŒëŠ” test acc = 0.9922 ì¸ë° 1ê°œë¡œ ì¤„ì–´ë“¤ë©´ test acc ë„ ì¡°ê¸ˆ ë–¨ì–´ì§„ë‹¤.
3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.
--> ì˜¤íˆë ¤ test acc ê°€ ì¡°ê¸ˆ ì¤„ì–´ë“¤ì—ˆë‹¤. (ì•„ë˜ í‘œ3 ì°¸ê³ )
4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it.
5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here! 
--> ok. got it.

```python
#The convolution
import tensorflow as tf
print(tf.__version__)
mnist = tf.keras.datasets.mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images=training_images.reshape(60000, 28, 28, 1)
training_images=training_images / 255.0
test_images = test_images.reshape(10000, 28, 28, 1)
test_images=test_images/255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.MaxPooling2D(2, 2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=10)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(test_acc)
```

**í‘œë¡œ ê²°ê³¼ ì •ë¦¬**
1. change the number of fillter   

|Conv ê°¯ìˆ˜|Conv fillter ê°¯ìˆ˜|epochs|traim acc|test acc|
|------|---|---|---|---|
|2|16|10|0.9968|0.9899|
|2|32|10|<span style="color:red">0.9977</span>|<span style="color:red">0.9922</span>|
|2|64|10|0.9972|0.9916|

2. delete final conv   

|Conv ê°¯ìˆ˜|Conv fillter ê°¯ìˆ˜|epochs|traim acc|test acc|
|------|---|---|---|---|
|1|32|10|<span style="color:red">0.9980</span>|0.9876|
|2|32|10|0.9977|<span style="color:red">0.9922</span>|

3. add more conv layer   

|Conv ê°¯ìˆ˜|Conv fillter ê°¯ìˆ˜|epochs|traim acc|test acc|
|------|---|---|---|---|
|1|32|10|<span style="color:red">0.9980</span>|0.9876|
|2|32|10|0.9977|<span style="color:red">0.9922</span>|
|3|32|10|0.9925|0.9851|

5. 
```python
import tensorflow as tf
print(tf.__version__)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if logs.get('accuracy') > 0.995:
      print("\nArrived to 99.5% accuracy so cancelling training!")
      self.model.stop_training = True

mnist = tf.keras.datasets.mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images=training_images.reshape(60000, 28, 28, 1)
training_images=training_images / 255.0
test_images = test_images.reshape(10000, 28, 28, 1)
test_images=test_images/255.0

callbacks = myCallback()

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.MaxPooling2D(2, 2),
  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=10, callbacks = [callbacks])
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(test_acc)
```
### Experiment with filters and pools (Lab 2)

>To try this notebook for yourself, and play with some convolutions, hereâ€™s the notebook. Let us know if you come up with any interesting filters of your own! 

[link](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W3/ungraded_labs/C1_W3_Lab_2_exploring_convolutions.ipynb)

>As before, spend a little time playing with this notebook. Try different filters, and research different filter types. There's some fun information about them here: https://lodev.org/cgtutor/filtering.html

## Quiz

1. What is a Convolution?

A technique to make images smaller
A technique to filter out unwanted images
A technique to make images bigger
**A technique to isolate features in images**

2. What is a Pooling?

A technique to isolate features in images
**A technique to reduce the information in an image while maintaining features**
A technique to combine pictures
A technique to make images sharper

3. How do Convolutions improve image recognition?

They make the image smaller
They make processing of images faster
**They isolate features in images**
They make the image clearer

4. After passing a 3x3 filter over a 28x28 image, how big will the output be?

28x28
**26x26**
25x25
31x31

5. After max pooling a 26x26 image with a 2x2 filter, how big will the output be?

28x28
**13x13**
26x26
56x56

6. Applying Convolutions on top of our Deep neural network will make training:

**It depends on many factors. It might make your training faster or slower, and a poorly designed Convolutional layer may even be less efficient than a plain DNN!**
Slower
Faster
Stay the same


# Week 4.
## lectures
### Understanding ImageGenerator   

```python
from tensorflow.keras.preprocessing.image
import ImageDataGenerator

#<train generator>
#It's a common mistake that people point the generator at the sub-directory.
#It will fail in that circumstance. You should always point it at the directory
#that contains sub-directories that contain your images.

train_dagagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
train_dir, #subdirectory ì•„ë‹ˆê³  directoryë¡œ ì œëŒ€ë¡œ ì„¤ì •
target_size=(300,300), #íŠ¸ë ˆì´ë‹ì„ ìœ„í•´ input dataê°€ ëª¨ë‘ ê°™ì€ sizeì—¬ì•¼ í•œë‹¤. ê·¸ë˜ì„œ ì´ ì˜µì…˜ ë„£ì–´ì„œ ë¡œë“œí•  ë•Œ ì•Œì•„ì„œ ë§ì¶°ì¤„ ìˆ˜ ìˆë‹¤.
batch_size = 128, #ì´ë¯¸ì§€ë“¤ì€ íŠ¸ë ˆì´ë‹ê³¼ ë²¨ë¦¬ë°ì´ì…˜ì—ì„œ batchë¡œ ì œê³µëœë‹¤.
class_mode = 'binary')

#<validation generator>
#training dategenê³¼ ë™ì¼í•˜ì§€ë§Œ í•œ ê°€ì§€! test imagesë“¤ì´ ë“¤ì–´ìˆëŠ” subdirectoryë¥¼ ì§€ë‹ˆê³  ìˆëŠ” validation directoryë¥¼ points atí•œë‹¤.
test_dagagen = ImageDataGenerator(rescale=1./255)
validation_generator = test_datagen.flow_from_directory(
validation_dir,  #ì´ ë¶€ë¶„ì—ì„œ ì°¨ì´!! (ì—­ì‹œë‚˜ subdirectoryì•„ë‹ˆê³  directoryë¡œ ì„¤ì •í•´ì£¼ì–´ì•¼í•œë‹¤.)
target_size=(300,300),
batch_size=32,
class_mode ='binary')
```

### Defining a ConvNet to use complex images

```python
# Here is the code
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)), #298,298,16,448(param)
  tf.keras.layers.MaxPooling2D(2, 2), #149,149,16,0
  tf.keras.layers.Conv2D(32, (3,3), activation='relu'), #147,147,32,4640
  tf.keras.layers.MaxPooling2D(2,2), #73,73,32,0
  tf.keras.layers.Conv2D(64, (3,3), activation='relu'), #71,71,64,18496
  tf.keras.layers.MaxPooling2D(2,2), #35,35,64,0
  tf.keras.layers.Flatten(), #78400,,,0
  tf.keras.layers.Dense(512, activation='relu'), #512,,,40141312
  tf.keras.layers.Dense(1, activation='sigmoid') #binaryì—ì„œ ì“°ê¸° í¸ë¦¬í•˜ë‹¤. #1,513
])

#total params: 40,165,409
#trainable params:40,165,409
#nontrainable params:0
```

### Training the ConvNet with fit_generator

```python
from tensorflow.keras.optimizers import RMSprop

model.compile(loss = 'binary_crossentropy',
    optimizer = RMSprop(lr=0.001),
    metrics = ['acc'])
```

* now we gonna use `model.fit_generator`, not model.fit : ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ë°ì´í„°ì…‹ ëŒ€ì‹ ì— a generatorë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. (ì•ì„œì„œ ë³¸ ì´ë¯¸ì§€ ì œë„ˆë ˆì´í„°)

```python
history = model.fit_generator(
    train_generator, #training direcì—ì„œ ì´ë¯¸ì§€ë“¤ì„ ëŒì–´ì˜¨ë‹¤.
    steps_per_epoch = 8, #ì „ì²´ batch ë¬¶ìŒì´ ëª‡ ê°œ ì¸ê°€. 1024ê°œì˜ train ì´ë¯¸ì§€ë¥¼ 128ê°œì”© ë¬¶ì–´ì„œ 1 batchë¡œ ê°€ì ¸ì˜¤ë‹ˆê¹Œ 8ê°œ ë¬¶ìŒì´ë‹¤.
    epochs = 15, #íŠ¸ë ˆì´ë‹ epochs
    validation_data = validation_generator, #validation setë„ ì–´ë””ì„œ ê°€ì ¸ì˜¤ëŠ”ì§€ ì •í•œë‹¤.
    validation_steps = 8, #256ê°œì˜ validationì´ë¯¸ì§€ê³  32ê°œì”© ë¬¶ì–´ì„œ 1 batchë¥¼ ê°€ì ¸ì™€ì„œ 8ê°œ ë¬¶ìŒìœ¼ë¡œ ë§Œë“ ë‹¤.
                          #'handle them in batches of 32 so we will do 8 steps'
    verbose =2 # how much to display while training is going on
)
```


* Once the model is trained, you will, of course, want to do some predictions on the model.

```python
import numpy as np
from google.colab import files
from keras.preprocessing import image

uploaded = files.upload()

for fn in uploaded.keys():

    #predicting images
    path= '/content/' + fn
    img = image.load_img(path,target_size=(300,300)) #ì¸í’‹ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆê°€ íŠ¸ë ˆë‹ˆì´í•  ë•Œì™€ ê°™ê²Œ ë§ì¶”ê¸°
    x=image.img_to_array(img)
    x=np.expand_dims(x,axis=0)

    images=np.vstack([x])
    classes = model.predict(image, batch_size=10)
    print(classes[0])
    if classes[0]>0.5:
        print(fn+"is a human")
    else:
        print(fn + "is a horse")
```

### Walking through developing a ConvNet

### Experiment with the horse or human classifier (Lab 1) 
[**Lab link**](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_1_image_generator_no_validation.ipynb)

### Adding automatic validation to test accuracy

### Get hands-on and use validation (Lab 2)
[**Lab link**](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_2_image_generator_with_validation.ipynb)

### Exploring the impact of compressing images
* ì´ë¯¸ì§€ë¥¼ ë” ì‘ê²Œ ì¤„ì˜€ì„ ë•Œì˜ ì˜í–¥ í™•ì¸í•˜ê¸°
>So we had quite a few convolutional layers to reduce the images down to condensed features. Now, this of course can slow down the training. So let's take a look at what would happen if we change it to a 150 by a 150 for the images to have a quarter of the overall data and to see what the impact would be.

ë’·ëª¨ìŠµì˜ íŒŒë€ë“œë ˆìŠ¤ ì—¬ìë¥¼ 150 by 150 í”½ì…€ë¡œ ì¤„ì¸ ì‚¬ì§„ì—ì„œëŠ” ë§ë¡œ ì¸ì‹í•˜ëŠ” ì˜¤ë¥˜ê°€ ë‚˜íƒ€ë‚¨.

> But now, she isn't. This is a great example  of the importance of measuring your training data against a large validation set, inspecting where it got it wrong and seeing what you can do to fix it. Using this smaller set is much cheaper to train, but then errors like this woman with her back turned and her legs obscured by the dress will happen, because we don't have that data in the training set. That's a nice hint about how to edit your dataset for the best effect in training.

ì¦‰ train ì„¸íŠ¸ë¥¼ ë” ëŠ˜ë ¤ì¤˜ì•¼í•œë‹¤ëŠ” ì´ì•¼ê¸°ì¸ê°€??


### Get Hands-on with compacted images (Lab 3)
[**Lab link**](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_3_compacted_images.ipynb)

## Quiz

1. Using Image Generator, how do you label images?   
Itâ€™s based on the directory the image is contained in
2. What method on the Image Generator is used to normalize the image?   
rescale
3. How did we specify the training size for the images?   
The target_size parameter on the training generator
4. When we specify the input_shape to be (300, 300, 3), what does that mean?   
Every Image will be 300x300 pixels, with 3 bytes to define color
5. If your training data is close to 1.000 accuracy, but your validation data isnâ€™t, whatâ€™s the risk here?   
Youâ€™re overfitting on your training data
6. Convolutional Neural Networks are better for classifying images like horses and humans because   
(1) In these images, the features may be in different parts of the frame
(2) Thereâ€™s a wide variety of horses
(3) Thereâ€™s a wide variety of humans
**(4) All of the above**
7. After reducing the size of the images, the training results were different. Why?   
(1)There was more condensed information in the images
(2)The training was faster
(3)There was less information in the images
**(4)We removed some convolutions to handle the smaller images***
